# Spacebar Helm Chart

> **Note:** This chart was generated by Cursor AI from a plan.

Helm chart for [Spacebar](https://spacebar.chat) - a Discord-compatible chat, voice and video platform. Deploys Spacebar server with optional [CloudNative-PG](https://cloudnative-pg.io/) PostgreSQL, default Traefik Ingress, and S3 CDN storage.

## Chart location

- **GitHub:** `https://github.com/<owner>/spacebarchart` (replace `<owner>` with your org or username)
- Chart path: `charts/spacebar`
- **Helm repo (use this for install):** `https://khaleb7.github.io/spacebarchart` (or `https://<owner>.github.io/spacebarchart` for your fork)

## Requirements

- Kubernetes 1.24+
- Helm 3.x
- Traefik Ingress Controller (default) or NGINX/ALB
- **cert-manager** is installed by default by this chart. **If cert-manager is already in the cluster, set `certManager.install: false`** or install will fail with CRD ownership errors.
- For PostgreSQL: [CloudNative-PG operator](https://cloudnative-pg.io/documentation/current/installation_upgrade/) installed (optionally via this chart with `postgresql.installOperator: true`)

## Installation

### Install from Helm repo (recommended)

Add this chart repo and install:

```bash
helm repo add spacebar https://khaleb7.github.io/spacebarchart
helm repo update
helm install spacebar spacebar/spacebar -n spacebar --create-namespace \
  --set ingress.host=spacebar.example.com \
  --set storage.bucket=my-bucket \
  --set storage.region=us-east-1
```

For your own fork, use `https://<owner>.github.io/spacebarchart` instead. The chart is published automatically on push to `main` via [release-charts.yml](../../.github/workflows/release-charts.yml).

**If cert-manager is already installed**, add `--set certManager.install=false`:

```bash
helm install spacebar spacebar/spacebar -n spacebar --create-namespace \
  --set certManager.install=false \
  --set ingress.host=spacebar.example.com \
  --set storage.bucket=my-bucket \
  --set storage.region=us-east-1
```

### Quick start (from source)

1. Install CloudNative-PG operator (if using in-cluster Postgres):

   ```bash
   helm repo add cnpg https://cloudnative-pg.github.io/charts
   helm install cnpg cnpg/cloudnative-pg -n cnpg-system --create-namespace
   ```

2. Clone the chart repo and install Spacebar:

   ```bash
   git clone https://github.com/<owner>/spacebarchart
   cd spacebarchart
   helm install spacebar ./charts/spacebar -n spacebar --create-namespace \
     --set ingress.host=spacebar.local \
     --set storage.bucket=my-spacebar-cdn \
     --set storage.region=us-east-1
   ```

   For S3 you must provide credentials via an existing secret (keys: `STORAGE_BUCKET`, `STORAGE_REGION`, and `AWS_ACCESS_KEY_ID`/`AWS_SECRET_ACCESS_KEY`, or use IRSA on EKS) and set `existingSecret` or create the secret manually and reference it.

### With in-chart CNPG operator (k3s)

From the repo root (after `git clone https://github.com/<owner>/spacebarchart`):

```bash
helm dependency update ./charts/spacebar
helm install spacebar ./charts/spacebar -n spacebar --create-namespace \
  --set postgresql.installOperator=true \
  --set ingress.host=spacebar.local \
  --set storage.bucket=my-bucket \
  --set storage.region=us-east-1
```

### Let's Encrypt (Traefik + cert-manager)

**cert-manager is installed by default** with this chart. Enable Let's Encrypt and set your ACME account email:

```bash
helm repo add spacebar https://khaleb7.github.io/spacebarchart
helm repo update
helm install spacebar spacebar/spacebar -n spacebar --create-namespace \
  --set ingress.host=spacebar.example.com \
  --set ingress.letsEncrypt.enabled=true \
  --set ingress.letsEncrypt.email=admin@example.com \
  --set ingress.letsEncrypt.createClusterIssuer=true \
  --set storage.bucket=my-bucket \
  --set storage.region=us-east-1
```

Set **certManager.install: false** if cert-manager is already installed in the cluster.

- **createClusterIssuer: true** — chart creates a ClusterIssuer (HTTP-01) for this release; no existing cert-manager issuer needed.
- **clusterIssuer: "letsencrypt-prod"** — use an existing ClusterIssuer; set this and leave **createClusterIssuer: false**.

### EKS (Terraform)

See [examples/eks](../../examples/eks) for a minimal EKS + Terraform example that installs this chart via `helm_release`.

## Configuration

| Key | Default | Description |
|-----|---------|-------------|
| `image.repository` | `ccgurley/spacebar-server` | Spacebar server image (official `spacebarchat/server` may be unavailable; override or build from [source](https://github.com/spacebarchat/server)) |
| `image.tag` | `latest` | Image tag |
| `replicaCount` | `1` | Number of replicas (scale >1 requires RabbitMQ) |
| `database.enabled` | `true` | Use PostgreSQL (CloudNative-PG Cluster or external) |
| `database.externalUrl` | — | External Postgres URL (in secret); use when not using CNPG Cluster |
| `postgresql.installOperator` | `false` | Install CloudNative-PG operator as subchart |
| `postgresql.cluster.instances` | `1` | CNPG Cluster instance count |
| `postgresql.cluster.storage.size` | `1Gi` | CNPG storage size |
| `storage.provider` | `s3` | CDN storage: `s3` or `file` |
| `storage.bucket` | `""` | S3 bucket name (required for S3) |
| `storage.region` | `""` | S3 region (required for S3) |
| `storage.existingClaim` | `""` | When `provider: file`, use this PVC name (e.g. NFS/RWX for multi-replica). Empty = chart creates a PVC. |
| `storage.useSharedVolume` | `false` | When `provider: file` and no `existingClaim`: if true, chart creates a ReadWriteMany (RWX) PVC (set `storageClass` to NFS or RWX provisioner). |
| `storage.storageClass` | `""` | Optional storage class for chart-created file PVC (e.g. `nfs-client` for NFS). |
| `ingress.enabled` | `true` | Create Ingress |
| `ingress.className` | `traefik` | Ingress class (Traefik, nginx, alb) |
| `ingress.host` | `spacebar.local` | Ingress host |
| `ingress.tls` | `[]` | TLS config (secretName, hosts); optional when using Let's Encrypt |
| `ingress.letsEncrypt.enabled` | `false` | Request TLS cert from Let's Encrypt via cert-manager |
| `ingress.letsEncrypt.email` | `""` | ACME account email (required when Let's Encrypt enabled) |
| `ingress.letsEncrypt.clusterIssuer` | `""` | Existing ClusterIssuer name (e.g. letsencrypt-prod); empty when using createClusterIssuer |
| `ingress.letsEncrypt.createClusterIssuer` | `false` | Create a ClusterIssuer (HTTP-01) for this release |
| `certManager.install` | `true` | Install cert-manager as a chart dependency (set false if already installed) |
| `existingSecret` | `""` | Existing secret for DATABASE, S3, JWT, etc. |

Public endpoints (`api_endpointPublic`, `cdn_endpointPublic`, `gateway_endpointPublic`) are derived from `ingress.host` and `ingress.tls` and written into the config file at `CONFIG_PATH` for client discovery.

## Storage (S3)

The chart uses S3 as the default CDN storage backend. Spacebar stores **user uploads** in this bucket: avatars, attachments, emoji, guild icons, and other CDN-served assets. The API and Gateway read/write these objects via the Spacebar server process.

### Required configuration

- **`storage.bucket`** — S3 bucket name. Create the bucket in the same region as your cluster (or a region you accept for latency/cost).
- **`storage.region`** — AWS region of the bucket (e.g. `us-east-1`). Must match the bucket’s region.

### Credentials

- **EKS (recommended):** Use [IRSA](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IAM Role for Service Account). Attach an IAM policy to the Spacebar pod’s service account that allows `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, and `s3:ListBucket` on the bucket (and optionally the bucket prefix). No `AWS_ACCESS_KEY_ID` or `AWS_SECRET_ACCESS_KEY` in a secret is needed; the chart can still set `STORAGE_BUCKET` and `STORAGE_REGION` via ConfigMap or values.
- **Static credentials:** Create a Kubernetes secret with keys `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` (and optionally `STORAGE_BUCKET`, `STORAGE_REGION`). Set `existingSecret` to that secret’s name. The IAM user or role must have the same S3 permissions as above.

### Implications

| Topic | Implication |
|-------|-------------|
| **Bucket ownership** | The chart does not create the bucket. Create it (e.g. Terraform, AWS Console) and ensure the Spacebar workload has IAM permission to read/write. |
| **Multi-replica** | If you scale Spacebar replicas beyond 1, all pods must use the same bucket (and region). S3 is shared; no extra config needed. |
| **Cost** | Storage and request costs apply. Consider S3 lifecycle rules (e.g. move old objects to Glacier) or size limits if you need to cap cost. |
| **Encryption** | Enable S3 server-side encryption (SSE-S3 or SSE-KMS) on the bucket if you need encryption at rest. The chart does not configure this. |
| **CORS** | If clients upload directly to S3 (Spacebar may serve uploads via the API instead), configure CORS on the bucket. For typical Spacebar usage (upload through the API), CORS may not be required. |
| **Existing data** | Switching from file storage to S3 (or vice versa) requires migrating existing CDN assets; the chart does not migrate. |

### Using S3 for Postgres backups (CloudNative-PG)

When using the in-cluster CloudNative-PG Cluster (no `database.externalUrl`), you can send **Postgres backups and WAL** to the same S3 bucket (with a path prefix) or to a separate bucket. CloudNative-PG uses [Barman Cloud](https://cloudnative-pg.io/documentation/current/backup_barmanobjectstore/) for continuous physical backup and WAL archiving; this enables hot backups and Point-in-Time Recovery (PITR).

The chart does **not** configure backup by default. To enable backup to S3 you must add `spec.backup.barmanObjectStore` to the Cluster. Two options:

1. **Same bucket, different path** — Use the CDN bucket with a prefix, e.g. `s3://my-spacebar-bucket/backups/postgres`. One bucket, one set of IAM/credentials; ensure the CNPG Cluster’s service account or backup secret has `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, `s3:ListBucket` on that prefix (or the whole bucket).
2. **Separate bucket** — Use a dedicated backup bucket, e.g. `s3://my-spacebar-backups`. Requires a second bucket and credentials (or IRSA) for the CNPG backup job.

**Requirements:**

- Postgres image must include Barman Cloud tools. Use the image `ghcr.io/cloudnative-pg/postgresql` (or a derivative that includes `barman-cli-cloud`). Override the Cluster’s image via `postgresql.cluster.image` if your chart version exposes it, or patch the Cluster after install.
- A Kubernetes secret with S3 credentials (e.g. `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`) or IRSA for the CNPG backup job.
- The Cluster CR must include a `backup.barmanObjectStore` block (see below). The chart does not currently add this from values; you can patch the Cluster or use a post-install hook.

**Example: patch the Cluster to add backup to S3**

After installing the chart, patch the Cluster to set the backup destination (same bucket as CDN, with prefix):

```yaml
# backup-patch.yaml
spec:
  backup:
    barmanObjectStore:
      destinationPath: "s3://MY_CDN_BUCKET/backups/postgres"
      s3Credentials:
        accessKeyId:
          name: spacebar-s3-creds   # secret with AWS_ACCESS_KEY_ID
          key: AWS_ACCESS_KEY_ID
        secretAccessKey:
          name: spacebar-s3-creds
          key: AWS_SECRET_ACCESS_KEY
      retentionPolicy: "30d"
```

Apply the patch (replace the cluster name and bucket if needed):

```bash
kubectl patch cluster -n spacebar spacebar-postgresql --type merge --patch-file backup-patch.yaml
```

Use the same secret as for Spacebar S3 (CDN) if using the same bucket and credentials; otherwise create a dedicated secret for backups.

**Optional: scheduled backups**

To run backups on a schedule, create a `ScheduledBackup` that targets the same Cluster and uses the same object store (the Cluster’s `barmanObjectStore` destination). Example (run daily at 2:00 UTC):

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: ScheduledBackup
metadata:
  name: spacebar-daily
  namespace: spacebar
spec:
  schedule: "0 2 * * *"   # cron: daily at 02:00 UTC
  cluster:
    name: spacebar-postgresql
  backupOwnerReference: self
  target: primary
```

See [CloudNative-PG Backup](https://cloudnative-pg.io/documentation/current/backup_barmanobjectstore/) and [Scheduling Backups](https://cloudnative-pg.io/documentation/current/scheduling_backup/) for retention, compression, and recovery.

### When you can’t use S3 (local or no cloud object store)

Spacebar supports **two** CDN storage providers ([env docs](https://docs.spacebar.chat/setup/server/configuration/env)):

| Provider | Use case | Chart config |
|----------|----------|--------------|
| **`file`** | Local path on disk. Good for **local dev**, single-node, or when you have no S3/compatible object store. | `storage.provider: file`; optional `storage.existingClaim` or let the chart create a PVC. Data lives at `STORAGE_LOCATION` (default `/data`). |
| **`s3`** | AWS S3 (or S3-compatible API). Default in this chart. | `storage.provider: s3`, `storage.bucket`, `storage.region`; credentials via secret or IRSA. |

There is **no** built-in provider for “plain” local file serving without the `file` backend: if you can’t use S3, use **file** storage.

**File storage (local or NFS):**

- Set `storage.provider: file`. Either:
  - **Chart-created PVC:** Leave `storage.existingClaim` unset. The chart creates a PVC. By default it uses **ReadWriteOnce** (single replica). For **multi-replica** with a shared volume (e.g. NFS), set `storage.useSharedVolume: true` and `storage.storageClass` to your NFS (or other RWX) provisioner (e.g. `nfs-client`, `nfs`); the chart will create a **ReadWriteMany** PVC.
  - **Existing PVC (e.g. NFS):** Set `storage.existingClaim` to the name of an existing PersistentVolumeClaim. Use a PVC backed by **NFS** or any **ReadWriteMany** volume so multiple Spacebar replicas can share the same CDN data. Create the PV/PVC (or use your NFS provisioner) outside the chart, then reference it here.
- The server writes CDN assets to a volume at `STORAGE_LOCATION` (chart default `/data`). No bucket or region; no S3 credentials required.

**S3-compatible backends (MinIO, DigitalOcean Spaces, etc.):**

- Spacebar’s env only documents `STORAGE_PROVIDER`, `STORAGE_BUCKET`, and `STORAGE_REGION`. If the server uses the AWS S3 SDK and supports a custom endpoint (e.g. via env or config), you can try `provider: s3` with that endpoint and the same bucket/region/credentials pattern; check the [Spacebar server](https://github.com/spacebarchat/server) source for endpoint/region overrides. This chart does not expose a custom S3 endpoint; you can pass it via `env` or `existingSecret` if the server supports it.

### NFS and POSIX / durability

| Use | NFS OK? | Notes |
|-----|---------|--------|
| **CDN file storage** (Spacebar uploads) | **Yes** | No strict fsync/durability requirements; NFS or any RWX volume is fine. |
| **Postgres (CNPG Cluster)** | **Only if configured correctly** | PostgreSQL relies on `fsync` for durability. NFS can break that if the server or client caches writes. |

**Postgres on NFS – requirements:**

- **NFS server:** Export with **`sync`** so the server flushes data to storage before replying. Without this, a client `fsync()` can succeed while data is still in the server’s cache, leading to data loss on server crash.
- **NFS client (Kubernetes mount):** Mount with **`hard`** (mandatory). Soft timeouts can cause I/O errors that PostgreSQL cannot recover from.
- In Kubernetes, set **`mountOptions`** on the PV (or in the StorageClass used by CNPG) to include `hard` and any other options you need. The NFS server export must be configured with `sync` on the NFS server itself; the chart cannot set that.

If you cannot guarantee sync server-side and hard client-side, prefer **block storage** (e.g. EBS, local PV) for the CNPG Cluster and use NFS only for CDN file storage.

## Troubleshooting

| Error | Fix |
|-------|-----|
| `CustomResourceDefinition ... exists and cannot be imported ... invalid ownership metadata; label validation error: missing key "app.kubernetes.io/managed-by"` | cert-manager is already installed in the cluster. Install with **`--set certManager.install=false`** so the chart does not try to manage cert-manager CRDs. |
| `ErrImagePull` / `ImagePullBackOff` for `spacebarchat/server:latest: not found` | The default image is now `ccgurley/spacebar-server:latest`. If you overrode to `spacebarchat/server` and it fails, use **`--set image.repository=ccgurley/spacebar-server`** or build from [source](https://github.com/spacebarchat/server) and set `image.repository` to your image. |

## Uninstall

```bash
helm uninstall spacebar -n spacebar
```

If you used `postgresql.installOperator: true`, the CNPG operator is in `cnpg-system`; remove it separately if desired.

## Links

- [Spacebar](https://spacebar.chat) / [GitHub](https://github.com/spacebarchat)
- [Spacebar server setup](https://docs.spacebar.chat/setup/server/)
- [CloudNative-PG](https://cloudnative-pg.io/)
